{
  "cells": [
    {
      "metadata": {
        "_uuid": "a25dad79d0e0ac40da4ab1ffa5bf3e24008ca367"
      },
      "cell_type": "markdown",
      "source": "# Sample Inflation\nThis competition evaluates only head of household prediction. In my previous trial, I found that prediction with non-head of household samples is much inferier than one without non-head of household. \n\nHowever, after dropping non-head of household samples, the dataset size decreases to one third. Sample size is a matter for the prediction accuracy. Then, I tried to leave some samples which have properties like head of household.\n\nMy procedure is follwing ...\n1. Generate a model to calssify wheather a sample is head or non-head of household\n2. Output probabilities that the sample is head of household\n3. Leave high probability samples \n\n"
    },
    {
      "metadata": {
        "_uuid": "abcfb65838905934cd284efb44126ff5a08ffe75"
      },
      "cell_type": "markdown",
      "source": "# result\n* Socre was not improvement"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_dir = '../input/'\nworking_dir = '../working/'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9632d88df34a7b6eed6332ca307520251a697eb0"
      },
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d041cc6d400253f8d3cc705f8d5c4497a6f4d6d",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(os.path.join(input_dir, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_dir, 'test.csv'))\n\n# Set index\ntrain.index = train['Id'].values\ntest.index = test['Id'].values\n\n# Pick Target\ntrain_target = train['Target']\n\n# Union train and test\nall_data = pd.concat([train.drop('Target', axis=1), test], axis=0)\n\nprint(train.shape)\nprint(test.shape)\nprint(all_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75c11ded696310ea36bf203b715f29209f85aa26"
      },
      "cell_type": "code",
      "source": "# data cleaning\n# copy from https://www.kaggle.com/katacs/data-cleaning-and-random-forest\n# and make a change it\ndef data_cleaning(data):\n    data['dependency']=np.sqrt(data['SQBdependency'])\n    data['rez_esc']=data['rez_esc'].fillna(0)\n    data['v18q1']=data['v18q1'].fillna(0)\n    data['v2a1']=data['v2a1'].fillna(0)\n    \n    data['edjefa'] = data['edjefa'].replace({'no': 0, 'yes': 1})\n    data['edjefa'] = data['edjefa'].astype('int')\n    data['edjefe'] = data['edjefe'].replace({'no': 0, 'yes': 1})\n    data['edjefe'] = data['edjefe'].astype('int')\n    meaneduc_nan=data[data['meaneduc'].isnull()][['Id','idhogar','escolari']]\n    me=meaneduc_nan.groupby('idhogar')['escolari'].mean().reset_index()\n    for row in meaneduc_nan.iterrows():\n        idx=row[0]\n        idhogar=row[1]['idhogar']\n        m=me[me['idhogar']==idhogar]['escolari'].tolist()[0]\n        data.at[idx, 'meaneduc']=m\n        data.at[idx, 'SQBmeaned']=m*m\n        \n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df5e868f140eceff0c6b9af50254b0f1d24ce84b"
      },
      "cell_type": "code",
      "source": "# train = data_cleaning(train)\n# test = data_cleaning(test)\nall_data = data_cleaning(all_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4f45c1b8d27f699eb1eac3cd8d05a6dd84db46d"
      },
      "cell_type": "code",
      "source": "# Tranform One-hot variables into Categorical variables\ndef onehot2cat(data, cat_col_new, cat_col_olds):\n    cat_col = data[cat_col_olds].idxmax(1)\n    cat_col.name = cat_col_new\n    cat_col = cat_col.astype('category')\n    data = pd.concat([data, cat_col], axis=1)\n    data = data.drop(cat_col_olds, axis=1)\n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5693abf24771f907974986c3fc22866ab21a1588"
      },
      "cell_type": "code",
      "source": "cat_col_new_list = ['pared', 'piso', 'techo', 'abastagua', 'sanitario',\n                'energcocinar', 'elimbasu', 'estadocivil',\n                'parentesco', 'tipovivi', 'lugar', 'area']\ncat_col_dict = {}\nfor cat_col_new in cat_col_new_list:\n    cat_col_olds = [s for s in train.columns.tolist() if s.startswith(cat_col_new)]\n    cat_col_dict[cat_col_new] = cat_col_olds\n    \ncat_col_dict['electricity'] = ['public', 'planpri', 'noelec', 'coopele']\ncat_col_dict['sex'] = ['male', 'female']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f757cd047fa39ffddba8feaaaf99e1d3f37854eb"
      },
      "cell_type": "code",
      "source": "for cat_col_new, cat_col_olds in cat_col_dict.items():\n    print(cat_col_olds)\n#     train = onehot2cat(train, cat_col_new, cat_col_olds)\n#     test = onehot2cat(test, cat_col_new, cat_col_olds)\n    all_data = onehot2cat(all_data, cat_col_new, cat_col_olds)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24b29b1875e7ba44bc77e79bb227425fb25a46cf"
      },
      "cell_type": "code",
      "source": "# Encode one-hot variables into numeric\n# like (bad, regular, good) -> (0 ,1, 2)\ndef onehot2num(data, status_col_new, status_col_olds):\n    status_df = data[status_col_olds]\n    status_df.columns = list(range(len(status_col_olds)))\n    num_col = status_df.idxmax(1)\n    num_col.name = status_col_new\n    data = pd.concat([data, num_col], axis=1)\n    data = data.drop(status_col_olds, axis=1)\n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "736fb171f7acf83c7b457dbd825b234dde0bc2b8"
      },
      "cell_type": "code",
      "source": "status_col_new_list = ['epared', 'etecho', 'eviv', 'instlevel']\nstatus_col_dict = {}\nfor status_col_new in status_col_new_list:\n    status_col_olds = [s for s in train.columns.tolist() if s.startswith(status_col_new)]\n    status_col_dict[status_col_new] = status_col_olds",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6c8aaf162b3babb766d29ed38d7f85278abcd84"
      },
      "cell_type": "code",
      "source": "for status_col_new, status_col_olds in status_col_dict.items():\n    print(status_col_olds)\n#     train = onehot2num(train, status_col_new, status_col_olds)\n#     test = onehot2num(test, status_col_new, status_col_olds)\n    all_data = onehot2num(all_data, status_col_new, status_col_olds)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f9a9ca4f46c2a659fe15c2e444b496b015bf074"
      },
      "cell_type": "code",
      "source": "# Delete needless columns\nneedless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone']\nSQB_cols = [s for s in train.columns.tolist() if 'SQB' in s]\nneedless_cols.extend(SQB_cols)\n# train = train.drop(needless_cols, axis=1)\n# test = test.drop(needless_cols, axis=1)\nall_data = all_data.drop(needless_cols, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b491892f67d35412857245b2430818ba301d5559"
      },
      "cell_type": "code",
      "source": "# Encode overcrowdig variables into crowding rate\nall_data['hacdor_rate'] = all_data['hogar_total'] / all_data['bedrooms']\nall_data['hacapo_rate'] = all_data['hogar_total'] / all_data['rooms']\nall_data = all_data.drop(['hacdor', 'hacapo'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc6bc04cf22047e03e16f3f384612fc193288f3a"
      },
      "cell_type": "code",
      "source": "# Encode dummy variables into category type\ndummy_col_list = ['cielorazo', 'dis', 'computer', 'television']\nfor dummy_col in dummy_col_list:\n#     train[dummy_col] = train[dummy_col].astype('category')\n#     test[dummy_col] = test[dummy_col].astype('category')\n    all_data[dummy_col] = all_data[dummy_col].astype('category')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e4250bde94328d69b6bd1c9c8fcf8bdc9caa862"
      },
      "cell_type": "code",
      "source": "all_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e765d41e642674bd82df3597e3a5bbcbf9a948e"
      },
      "cell_type": "markdown",
      "source": "## Select samples like head of household"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ab744ba9dc5a18a7b2c2f7a7e787a12069497fd"
      },
      "cell_type": "code",
      "source": "head_df_y = all_data['parentesco']=='parentesco1'\n\ncols_relate_head =['edjefa', 'edjefe']\nhead_df_X = all_data.drop(['parentesco', 'Id', 'idhogar'] + cols_relate_head, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "682241c47adecbe0397e9df05bb7b96b75e4dbe3",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, make_scorer\nimport lightgbm as lgb\n\nauc_scorer = make_scorer(auc, greater_is_better=True)\n\ngbm_cv_param = {\n    'num_leaves':[12]\n#     ,'min_data_in_leaf':[9]\n#     ,'max_depth':[14]\n}\ngbm_cv = GridSearchCV(\n    lgb.LGBMClassifier(objective='binary', class_weight='balanced', seed=0)\n    , gbm_cv_param\n    , scoring=auc_scorer\n    , cv=5\n)\ngbm_cv.fit(head_df_X, head_df_y)\ngbm_cv.best_params_\n\n\n# params = {'num_leaves': 13, 'min_data_in_leaf': 23, 'max_depth': 11, 'learning_rate': 0.09, 'feature_fraction': 0.74}\n# gbm = lgb.LGBMClassifier(objective='multiclassova', class_weight='balanced', random_state=0)\n# gbm.set_params(**params)\n# gbm.fit(head_df_X, head_df_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb0fb07fe46a5ee2573d5ea16019de9244ceeaa0"
      },
      "cell_type": "code",
      "source": "# params = {'num_leaves': 13, 'min_data_in_leaf': 23, 'max_depth': 11, 'learning_rate': 0.09, 'feature_fraction': 0.74}\ngbm = lgb.LGBMClassifier(objective='binary', class_weight='balanced', random_state=0)\ngbm.set_params(**(gbm_cv.best_params_))\ngbm.fit(head_df_X, head_df_y)\ny_pred_prob = gbm.predict_proba(head_df_X)[:,1]\ny_pred = y_pred_prob > 0.5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55c757bb2f18763262b8234fb876190889982913"
      },
      "cell_type": "code",
      "source": "cm = confusion_matrix(head_df_y, y_pred)\ncm_df_columns = ['pred_'+str(i) for i in range(cm.shape[0])]\ncm_df_index = ['true_'+str(i) for i in range(cm.shape[0])]\ncm_df = pd.DataFrame(data=cm, columns=cm_df_columns, index=cm_df_index)\n\nac = accuracy_score(head_df_y, y_pred)\nfpr, tpr, thresholds = roc_curve(head_df_y, y_pred_prob)\nauc_score = auc(fpr, tpr)\nprint(\"confusion matrix: \\n\", cm_df)\nprint(\"accuracy score: \\n\", ac)\nprint(\"auc score: \\n\", auc_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e49c052b4ad403d92df671cad0e2feed674d6cd2"
      },
      "cell_type": "code",
      "source": "ft_imp = pd.DataFrame({'features':head_df_X.columns,\n                       'importance':gbm.feature_importances_})\nft_imp.sort_values(by='importance', ascending=False).iloc[0:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0621a86332f3c4263d091ea59b58b36412a5e7d1"
      },
      "cell_type": "code",
      "source": "pred_df = pd.DataFrame({'Id':all_data['Id'],\n                            'true':head_df_y,\n                            'pred':y_pred\n                           })\nheadish_idx =  pred_df.query('true==0 & pred==1')['Id']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9534d0fc6dcd522035e652018f1a4e2baca5355"
      },
      "cell_type": "code",
      "source": "headish_idx_train = []\nfor train_id in train['Id']:\n    if train_id in headish_idx:\n        headish_idx_train.append(train_id)\n\nprint('additional sample size:{}'.format(len(headish_idx_train)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf976c03d25d41b8ea09160786fd4db5048743e2"
      },
      "cell_type": "markdown",
      "source": "## Simple LightGBM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b2bd93f64abae9b6f271fdf668950c2d73f09b3"
      },
      "cell_type": "raw",
      "source": "train_head = all_data.loc[train.index]\ntrain_head['Target'] = train_target\ntrain_head = train_head.query('parentesco==\"parentesco1\"')\n\ntrain_headish = all_data.loc[train.index]\ntrain_headish['Target'] = train_target\ntrain_headish.head()\ntrain_headish = train_headish.loc[headish_idx_train]\n\n# union household head like samples\ntrain = pd.concat([train_head, train_headish], axis=0)\ntrain = train.drop('parentesco', axis=1)\ntrain.shape"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e059216006c4df5d2e7dbac4ef253effa75d24f"
      },
      "cell_type": "code",
      "source": "train_headish = all_data.loc[train.index]\ntrain_headish['Target'] = train_target\ntrain_headish.head()\ntrain_headish = train_headish.loc[headish_idx_train]\ntrain_headish = train_headish.drop('parentesco', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "441320be0d63c1b37899749354748ae89389ad0c"
      },
      "cell_type": "code",
      "source": "train = all_data.loc[train.index]\ntrain['Target'] = train_target\ntrain = train.query('parentesco==\"parentesco1\"')\ntrain = train.drop('parentesco', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79581a974385c1b8e88843b7e8ab18e9dc6cb854"
      },
      "cell_type": "code",
      "source": "test = all_data.loc[test.index]\ntest = test.drop('parentesco', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d60bc2fc0b86b1c935865dea92c3a5f874f1d64",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Split data\ntrain_Id = train['Id'] # individual ID\ntrain_idhogar = train['idhogar'] # household ID\ntrain_y = train['Target'] # Target value\ntrain_X = train.drop(['Id', 'Target', 'idhogar'], axis=1) # features\n\ntrain_headish_Id = train_headish['Id'] # individual ID\ntrain_headish_idhogar = train_headish['idhogar'] # household ID\ntrain_headish_y = train_headish['Target'] # Target value\ntrain_headish_X = train_headish.drop(['Id', 'Target', 'idhogar'], axis=1) # features\n\ntest_Id = test['Id'] # individual ID\ntest_idhogar = test['idhogar'] # household ID\ntest_X = test.drop(['Id', 'idhogar'], axis=1) # features\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27df44a94b52bc1cca4dd100823c6726fb7dea0e"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.1, random_state=0)\n\nX_train = pd.concat([X_train, train_headish_X], axis=0)\ny_train = pd.concat([y_train, train_headish_y], axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddc14d963f2d16e376f15d50557b848d0f9fd2ec"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, f1_score, make_scorer\nimport lightgbm as lgb\n\n\n\nF1_scorer = make_scorer(f1_score, greater_is_better=True, average='macro')\n\n# gbm_param = {\n#     'num_leaves':[210]\n#     ,'min_data_in_leaf':[9]\n#     ,'max_depth':[14]\n# }\n# gbm = GridSearchCV(\n#     lgb.LGBMClassifier(objective='multiclassova', class_weight='balanced', seed=0)\n#     , gbm_param\n#     , scoring=F1_scorer\n# )\n\n\n# params = {'num_leaves': 13, 'min_data_in_leaf': 23, 'max_depth': 11, 'learning_rate': 0.09, 'feature_fraction': 0.74}\ngbm = lgb.LGBMClassifier(objective='multiclassova', class_weight='balanced', random_state=0)\n# gbm.set_params(**params)\n\ngbm.fit(X_train, y_train)\n# gbm.best_params_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "458423a2b5c5151f1fb397c98cf3f2cc6a6d251a"
      },
      "cell_type": "code",
      "source": "import pickle\nwith open(os.path.join(working_dir, '20180822_lgbm_inflation.pickle'), mode='wb') as f:\n    pickle.dump(gbm, f)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e133dcb8f8c1eadf0503e3c5338c09fe89ce449"
      },
      "cell_type": "code",
      "source": "y_test_pred = gbm.predict(X_test)\ncm = confusion_matrix(y_test, y_test_pred)\nf1 = f1_score(y_test, y_test_pred, average='macro')\nprint(\"confusion matrix: \\n\", cm)\nprint(\"macro F1 score: \\n\", f1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cf36dc0a7f27dd680725f948d0ff809cbf07b49"
      },
      "cell_type": "code",
      "source": "gbm = lgb.LGBMClassifier(objective='multiclassova', class_weight='balanced', random_state=0)\ngbm.fit(train_X, train_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e25bb77387bb8c9773930f3f06871bdd70413d0"
      },
      "cell_type": "code",
      "source": "pred = gbm.predict(test_X)\npred = pd.Series(data=pred, index=test_Id.values, name='Target')\npred = pd.concat([test_Id, pred], axis=1, join_axes=[test_Id.index])\npred.to_csv('20180822_lgbm_inflation.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c433e1b0f8eb3df2a8b38bd449dc55e6f89023f6"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}